{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def490ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA PATHS\n",
    "'''\n",
    "TOP_DIR = '/tf/Notebooks/Iwashita'\n",
    "\n",
    "IR_PATH = TOP_DIR + '/Data/IR/'\n",
    "RGB_PATH = TOP_DIR + '/Data/RGB/'\n",
    "MASKS_PATH = TOP_DIR + '/Data/Masks/'\n",
    "ANNOTATIONS_PATH = TOP_DIR + '/Data/Annotations/'\n",
    "\n",
    "'''\n",
    "OUTPUTS PATH\n",
    "'''\n",
    "WEIGHTS_PATH = TOP_DIR + '/output/Weights/'\n",
    "METRICS_PATH = TOP_DIR + '/output/Metrics/'\n",
    "\n",
    "!cd $TOP_DIR && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "from numpy import asarray, save\n",
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib.pyplot import axis, figure, imshow, show, subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SUPPORTING FUNCTIONS\n",
    "'''\n",
    "\n",
    "'''\n",
    "POPULATE DATA SETS\n",
    "'''\n",
    "def populate_data_sets(X_list, y_list, rgb_dict, ir_dict, annotations_dict):\n",
    "    X_rgb = []\n",
    "    X_ir = []\n",
    "    y_data =[]\n",
    "\n",
    "    for i, fn in enumerate(X_list, start=0):        \n",
    "        X_rgb.append(rgb_dict[fn])\n",
    "        X_ir.append(ir_dict[fn])\n",
    "        y_data.append(annotations_dict[fn])\n",
    "\n",
    "    return np.array(X_rgb), np.array(X_ir), np.array(y_data)\n",
    "\n",
    "'''\n",
    "CALCULATE CLASS FREQUENCY\n",
    "'''\n",
    "def calculate_class_frequency_single(annotation_array):\n",
    "    num_classes = annotation_array.shape[2]\n",
    "    \n",
    "    pixel_count = np.zeros(num_classes)\n",
    "\n",
    "    # Iterate over each class and count pixels\n",
    "    for i in range(num_classes):\n",
    "        pixel_count[i] = np.sum(annotation_array[:, :, i] == 1)\n",
    "\n",
    "    # Compute class frequencies\n",
    "    class_frequency = pixel_count / np.sum(pixel_count)\n",
    "\n",
    "    # Print out the frequency for each class\n",
    "    for cls in classes:\n",
    "        print(f\"{cls.name}: {class_frequency[cls.value]*100:.4f}\")\n",
    "\n",
    "    return class_frequency\n",
    "\n",
    "def calculate_class_frequency_set(annotation_array):\n",
    "    num_classes = annotation_array.shape[3]\n",
    "    \n",
    "    pixel_count = np.zeros(num_classes)\n",
    "    \n",
    "    # Iterate over each class and count pixels\n",
    "    for i in range(num_classes):\n",
    "        pixel_count[i] = np.sum(annotation_array[:, :, :, i] == 1)\n",
    "        \n",
    "    # Compute class frequencies\n",
    "    class_frequency = pixel_count / np.sum(pixel_count)\n",
    "    \n",
    "    # Print out the frequency for each class\n",
    "    for cls in classes:\n",
    "        print(f\"{cls.name}: {class_frequency[cls.value]*100:.4f}\")\n",
    "        \n",
    "    return class_frequency\n",
    "\n",
    "'''\n",
    "ONE-HOT ANNOTATION CHECK\n",
    "'''\n",
    "def display_one_hot_annotation(annotations_onehot):\n",
    "    label = np.argmax(annotations_onehot, axis=-1)\n",
    "    cmap = plt.get_cmap('tab10', 7)\n",
    "\n",
    "    plt.imshow(label, cmap=cmap)\n",
    "    plt.colorbar(ticks=range(num_classes), format=plt.FuncFormatter(lambda val, loc: {\n",
    "        0: \"unlabeled\",\n",
    "        1: \"sand\",\n",
    "        2: \"soil\",\n",
    "        3: \"ballast\",\n",
    "        4: \"rock\",\n",
    "        5: \"bedrock\",\n",
    "        6: \"rocky terrain\"\n",
    "    }[val]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMAGE PROPERTIES - Original image dimensions are 800x600\n",
    "'''\n",
    "IMG_HEIGHT = 572\n",
    "IMG_WIDTH = 572\n",
    "\n",
    "RGB_CHANNELS = 3\n",
    "IR_CHANNELS = 1\n",
    "\n",
    "'''\n",
    "IMAGE LISTS\n",
    "'''\n",
    "img_list = [file for file in os.listdir(RGB_PATH) if file.lower().endswith('0000.png')]\n",
    "\n",
    "rgb_imgs = {}\n",
    "ir_imgs = {}\n",
    "\n",
    "'''\n",
    "LOAD MASKS\n",
    "'''\n",
    "rgb_mask = np.array(Image.open(os.path.join(MASKS_PATH, 'rgb_mask.ppm'))) / 255\n",
    "ir_mask = np.array(Image.open(os.path.join(MASKS_PATH, 'ir_mask.png')))[:,:,0] / 255\n",
    "\n",
    "'''\n",
    "LOAD AND NORMALIZE\n",
    "'''\n",
    "print(\"Processing RGB images...\")\n",
    "\n",
    "for n, filename in tqdm(enumerate(img_list, start=0), total=len(img_list)):\n",
    "    \n",
    "    # Open image from file\n",
    "    rgb_img = Image.open(os.path.join(RGB_PATH, filename))\n",
    "    \n",
    "    # Normalize RGB image in an 600x800x3 numpy array\n",
    "    rgb_array = np.array(rgb_img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Apply mask\n",
    "    rgb_array = rgb_array * rgb_mask\n",
    "    \n",
    "    # Resize\n",
    "    rgb_array = resize(rgb_array, (IMG_WIDTH, IMG_HEIGHT), mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    # Save to dictionary\n",
    "    rgb_imgs[filename] = rgb_array\n",
    "    \n",
    "print(\"Processing IR images...\")\n",
    "\n",
    "for n, filename in tqdm(enumerate(img_list, start=0), total=len(img_list)):\n",
    "    \n",
    "    # Open image from file\n",
    "    ir_img = Image.open(os.path.join(IR_PATH, filename))\n",
    "    \n",
    "    # Normalize IR image in an 600x800 numpy array\n",
    "    ir_array = np.array(ir_img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    # Apply mask\n",
    "    ir_array = ir_array * ir_mask\n",
    "    \n",
    "    # Resize\n",
    "    ir_array = resize(ir_array, (IMG_WIDTH, IMG_HEIGHT), mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    # Save to dictionary\n",
    "    ir_imgs[filename] = ir_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afec1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subplot(121), imshow(rgb_array), axis('off')\n",
    "subplot(122), imshow(ir_array), axis('off')\n",
    "\n",
    "print(rgb_array.shape)\n",
    "print(np.max(rgb_array))\n",
    "\n",
    "print(ir_array.shape)\n",
    "print(np.max(ir_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CLASSES\n",
    "'''\n",
    "classes = Enum('Classes', [\n",
    "    '__UNLABELED__',\n",
    "    'SAND',\n",
    "    'SOIL',\n",
    "    'BALLAST',\n",
    "    'ROCK',\n",
    "    'BEDROCK',\n",
    "    'ROCKY_TERRAIN'\n",
    "    ], start=0)\n",
    "\n",
    "num_classes = max(classes, key=lambda x: x.value).value + 1\n",
    "\n",
    "'''\n",
    "LOAD/DECODE ANNOTATION FILES\n",
    "'''\n",
    "annotations = {}\n",
    "\n",
    "print(\"Loading annotation files...\")\n",
    "\n",
    "for n, filename in tqdm(enumerate(img_list, start=0), total=len(img_list)):\n",
    "\n",
    "    img = Image.open(os.path.join(ANNOTATIONS_PATH, filename)).resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "\n",
    "    encoded = np.array(img)\n",
    "\n",
    "    label = np.bitwise_or(np.bitwise_or(\n",
    "        encoded[:, :, 0].astype(np.uint32),\n",
    "        encoded[:, :, 1].astype(np.uint32) << 8),\n",
    "        encoded[:, :, 2].astype(np.uint32) << 16)\n",
    "\n",
    "    annotations[filename] = label\n",
    "\n",
    "'''\n",
    "ONE-HOT ENCODE\n",
    "'''\n",
    "annotations_onehot = {}\n",
    "class_freq = {i: 0 for i in range(num_classes)}\n",
    "\n",
    "print(\"One-hot encoding annotation files...\")\n",
    "\n",
    "for n, filename in tqdm(enumerate(img_list, start=0), total=len(img_list)):\n",
    "\n",
    "    onehot_annotation = np.zeros((IMG_HEIGHT, IMG_WIDTH, num_classes), dtype=np.uint8)\n",
    "    \n",
    "    for c in range(num_classes):\n",
    "        mask = (annotations[filename] == c)\n",
    "        onehot_annotation[..., c] = mask\n",
    "        class_freq[c] += np.sum(mask)\n",
    "    \n",
    "    annotations_onehot[filename] = onehot_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8510c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pixels = sum(class_freq.values())\n",
    "\n",
    "class_percentages = {cls: (freq / total_pixels) * 100 for cls, freq in class_freq.items()}\n",
    "\n",
    "for cls, percentage in class_percentages.items():\n",
    "    if cls == 0:\n",
    "      print(\"\\n\")\n",
    "    if cls != 0:\n",
    "      print(f\"{classes(cls).name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a398204",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = annotations_onehot['03__2017-11-17-105825-0000.png']\n",
    "print(len(annotations_onehot))\n",
    "test = np.array(list(annotations_onehot.values()))\n",
    "print(test.shape[3])\n",
    "\n",
    "calculate_class_frequency_single(test[0])\n",
    "print(\"\")\n",
    "calculate_class_frequency_single(test[1])\n",
    "print(\"\")\n",
    "calculate_class_frequency_set(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae553cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_one_hot_annotation(onehot_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ab5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTER EXPERIMENT 1 DATA\n",
    "'''\n",
    "exp1_pattern = r'^\\d{2}__2017-11-17-16(4[0-9]|[4-5]\\d)[0-9]{2}-0000.png$'\n",
    "exp1_img_list = [file for file in img_list if re.match(exp1_pattern, file)]\n",
    "\n",
    "'''\n",
    "SPLIT DATA\n",
    "'''\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    exp1_img_list, \n",
    "    exp1_img_list, \n",
    "    test_size=0.50, \n",
    "    train_size=0.50, \n",
    "    random_state=42, \n",
    "    shuffle=True)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.50, \n",
    "    train_size=0.50, \n",
    "    random_state=42, \n",
    "    shuffle=False)\n",
    "\n",
    "print(\"Populating experiment 1 training data sets...\")\n",
    "exp1_rgb_X_train, exp1_ir_X_train, exp1_y_train = populate_data_sets(\n",
    "    X_train, y_train, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 1 validation data sets...\")  \n",
    "exp1_rgb_X_val, exp1_ir_X_val, exp1_y_val = populate_data_sets(\n",
    "    X_val, y_val, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 1 test data sets...\")  \n",
    "exp1_rgb_X_test, exp1_ir_X_test, exp1_y_test = populate_data_sets(\n",
    "    X_test, y_test, rgb_imgs, ir_imgs, annotations_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp1_y_train[0].shape)\n",
    "calculate_class_frequency_set(exp1_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584aca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP1_DIR = '/tf/Notebooks/Iwashita/Data/Preprocessed/Experiment1'\n",
    "\n",
    "'''\n",
    "SAVE TRAINING DATA\n",
    "'''\n",
    "save(EXP1_DIR + '/Train/exp1_rgb_X_train.npy', exp1_rgb_X_train)\n",
    "save(EXP1_DIR + '/Train/exp1_ir_X_train.npy', exp1_ir_X_train)\n",
    "save(EXP1_DIR + '/Train/exp1_y_train.npy', exp1_y_train)\n",
    "\n",
    "'''\n",
    "SAVE VALIDATION DATA\n",
    "'''\n",
    "save(EXP1_DIR + '/Validate/exp1_rgb_X_val.npy', exp1_rgb_X_val)\n",
    "save(EXP1_DIR + '/Validate/exp1_ir_X_val.npy', exp1_ir_X_val)\n",
    "save(EXP1_DIR + '/Validate/exp1_y_val.npy', exp1_y_val)\n",
    "\n",
    "'''\n",
    "SAVE TEST DATA\n",
    "'''\n",
    "save(EXP1_DIR + '/Test/exp1_rgb_X_test.npy', exp1_rgb_X_test)\n",
    "save(EXP1_DIR + '/Test/exp1_ir_X_test.npy', exp1_ir_X_test)\n",
    "save(EXP1_DIR + '/Test/exp1_y_test.npy', exp1_y_test)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTER EXPERIMENT 2 DATA\n",
    "'''\n",
    "exp2_img_list = img_list\n",
    "\n",
    "'''\n",
    "SPLIT DATA\n",
    "'''\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    exp2_img_list, \n",
    "    exp2_img_list, \n",
    "    test_size=0.50, \n",
    "    train_size=0.50, \n",
    "    random_state=42, \n",
    "    shuffle=True)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, \n",
    "    y_temp, \n",
    "    test_size=0.50, \n",
    "    train_size=0.50, \n",
    "    random_state=42, \n",
    "    shuffle=False)\n",
    "\n",
    "print(\"Populating experiment 2 training data sets...\")\n",
    "exp2_rgb_X_train, exp2_ir_X_train, exp2_y_train = populate_data_sets(\n",
    "    X_train, y_train, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 2 validation data sets...\")  \n",
    "exp2_rgb_X_val, exp2_ir_X_val, exp2_y_val = populate_data_sets(\n",
    "    X_val, y_val, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 2 test data sets...\")  \n",
    "exp2_rgb_X_test, exp2_ir_X_test, exp2_y_test = populate_data_sets(\n",
    "    X_test, y_test, rgb_imgs, ir_imgs, annotations_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70042a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP2_DIR = '/tf/Notebooks/Iwashita/Data/Preprocessed/Experiment2'\n",
    "\n",
    "'''\n",
    "SAVE TRAINING DATA\n",
    "'''\n",
    "save(EXP2_DIR + '/Train/exp2_rgb_X_train.npy', exp2_rgb_X_train)\n",
    "save(EXP2_DIR + '/Train/exp2_ir_X_train.npy', exp2_ir_X_train)\n",
    "save(EXP2_DIR + '/Train/exp2_y_train.npy', exp2_y_train)\n",
    "\n",
    "'''\n",
    "SAVE VALIDATION DATA\n",
    "'''\n",
    "save(EXP2_DIR + '/Validate/exp2_rgb_X_val.npy', exp2_rgb_X_val)\n",
    "save(EXP2_DIR + '/Validate/exp2_ir_X_val.npy', exp2_ir_X_val)\n",
    "save(EXP2_DIR + '/Validate/exp2_y_val.npy', exp2_y_val)\n",
    "\n",
    "'''\n",
    "SAVE TEST DATA\n",
    "'''\n",
    "save(EXP2_DIR + '/Test/exp2_rgb_X_test.npy', exp2_rgb_X_test)\n",
    "save(EXP2_DIR + '/Test/exp2_ir_X_test.npy', exp2_ir_X_test)\n",
    "save(EXP2_DIR + '/Test/exp2_y_test.npy', exp2_y_test)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54241b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILTER EXPERIMENT 3 DATA\n",
    "'''\n",
    "exp3_pattern = r'^\\d{2}__2017-11-17-(?:14(?:1[0-9]|[2-9][0-9])|15\\d{2}|16(?:[0-4][0-9]|5[0-9]))[0-5][0-9]-0000.png$'\n",
    "exp3_img_list = [file for file in img_list if re.match(exp3_pattern, file)]\n",
    "\n",
    "exp3_test_a_pattern = r'^\\d{2}__2017-11-17-(?:1(?:[0]\\d{2}|1(?:[0-9][0-9]|2(?:[0-9][0-9]|3[0-5][0-9]))))[0-5][0-9]-0000.png$'\n",
    "exp3_test_a_list = [file for file in img_list if re.match(exp3_test_a_pattern, file)]\n",
    "\n",
    "exp3_test_b_pattern = r'^\\d{2}__2017-11-17-(?:1(?:[4-6]\\d{2}|7(?:[0-2][0-9]|3[0-9])))[0-5][0-9]-0000.png$'\n",
    "exp3_test_b_list = [file for file in img_list if re.match(exp3_test_b_pattern, file)]\n",
    "\n",
    "'''\n",
    "SPLIT DATA\n",
    "'''\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    exp3_img_list, \n",
    "    exp3_img_list, \n",
    "    test_size=0.70, \n",
    "    train_size=0.30, \n",
    "    random_state=42, \n",
    "    shuffle=True)\n",
    "\n",
    "print(\"Populating experiment 3 training data sets...\")\n",
    "exp3_rgb_X_train, exp3_ir_X_train, exp3_y_train = populate_data_sets(X_train, y_train, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 3 validation data sets...\")  \n",
    "exp3_rgb_X_val, exp3_ir_X_val, exp3_y_val = populate_data_sets(X_val, y_val, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "\n",
    "print(\"Populating experiment 3 test data sets...\")  \n",
    "exp3_rgb_X_test_a, exp3_ir_X_test_a, exp3_y_test_a = populate_data_sets(exp3_test_a_list, exp3_test_a_list, rgb_imgs, ir_imgs, annotations_onehot)\n",
    "exp3_rgb_X_test_b, exp3_ir_X_test_b, exp3_y_test_b = populate_data_sets(exp3_test_b_list, exp3_test_b_list, rgb_imgs, ir_imgs, annotations_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP3_DIR = '/tf/Notebooks/Iwashita/Data/Preprocessed/Experiment3'\n",
    "\n",
    "'''\n",
    "SAVE TRAINING DATA\n",
    "'''\n",
    "save(EXP3_DIR + '/Train/exp3_rgb_X_train.npy', exp3_rgb_X_train)\n",
    "save(EXP3_DIR + '/Train/exp3_ir_X_train.npy', exp3_ir_X_train)\n",
    "save(EXP3_DIR + '/Train/exp3_y_train.npy', exp3_y_train)\n",
    "\n",
    "'''\n",
    "SAVE VALIDATION DATA\n",
    "'''\n",
    "save(EXP3_DIR + '/Validate/exp3_rgb_X_val.npy', exp3_rgb_X_val)\n",
    "save(EXP3_DIR + '/Validate/exp3_ir_X_val.npy', exp3_ir_X_val)\n",
    "save(EXP3_DIR + '/Validate/exp3_y_val.npy', exp3_y_val)\n",
    "\n",
    "'''\n",
    "SAVE TEST DATA\n",
    "'''\n",
    "save(EXP3_DIR + '/Test/exp3_rgb_X_test_a.npy', exp3_rgb_X_test_a)\n",
    "save(EXP3_DIR + '/Test/exp3_ir_X_test_a.npy', exp3_ir_X_test_a)\n",
    "save(EXP3_DIR + '/Test/exp3_y_test_a.npy', exp3_y_test_a)\n",
    "\n",
    "save(EXP3_DIR + '/Test/exp3_rgb_X_test_b.npy', exp3_rgb_X_test_b)\n",
    "save(EXP3_DIR + '/Test/exp3_ir_X_test_b.npy', exp3_ir_X_test_b)\n",
    "save(EXP3_DIR + '/Test/exp3_y_test_b.npy', exp3_y_test_b)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
