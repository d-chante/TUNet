{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff70fb9e",
   "metadata": {},
   "source": [
    "# Siamese TUNet BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52240bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "IMPORTS\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.image import resize, ResizeMethod\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Average, Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from numpy import load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import get_ipython\n",
    "from IPython.terminal.interactiveshell import TerminalInteractiveShell\n",
    "\n",
    "shell = TerminalInteractiveShell.instance()\n",
    "\n",
    "'''\n",
    "DATA PATHS\n",
    "'''\n",
    "TOP_DIR = '/tf/Notebooks/Iwashita'\n",
    "\n",
    "TRAIN_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment2/Train'\n",
    "VAL_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment2/Validate'\n",
    "TEST_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment2/Test'\n",
    "\n",
    "'''\n",
    "OUTPUTS PATH\n",
    "'''\n",
    "WEIGHTS_PATH = TOP_DIR + '/Output/Weights/'\n",
    "METRICS_PATH = TOP_DIR + '/Output/Metrics/'\n",
    "\n",
    "'''\n",
    "GPU\n",
    "'''\n",
    "gpu_p40 = '/device:GPU:1'\n",
    "gpu_1660 = '/device:GPU:0'\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "'''\n",
    "TRAINING DATA\n",
    "'''\n",
    "print(TRAIN_DIR)\n",
    "!cd /tf/Notebooks/Iwashita/Data/Preprocessed_wAugmentation/Experiment2/Train && ls\n",
    "\n",
    "exp2_rgb_X_train = load(TRAIN_DIR + '/exp2_rgb_X_train.npy')\n",
    "exp2_ir_X_train = load(TRAIN_DIR + '/exp2_ir_X_train.npy')\n",
    "exp2_y_train = load(TRAIN_DIR + '/exp2_y_train.npy')\n",
    "\n",
    "'''\n",
    "VALIDATION DATA\n",
    "'''\n",
    "exp2_rgb_X_val = load(VAL_DIR + '/exp2_rgb_X_val.npy')\n",
    "exp2_ir_X_val = load(VAL_DIR + '/exp2_ir_X_val.npy')\n",
    "exp2_y_val = load(VAL_DIR + '/exp2_y_val.npy')\n",
    "\n",
    "'''\n",
    "TEST DATA\n",
    "'''\n",
    "exp2_rgb_X_test = load(TEST_DIR + '/exp2_rgb_X_test.npy')\n",
    "exp2_ir_X_test = load(TEST_DIR + '/exp2_ir_X_test.npy')\n",
    "exp2_y_test = load(TEST_DIR + '/exp2_y_test.npy')\n",
    "\n",
    "'''\n",
    "INTERSECTION OVER UNION\n",
    "'''\n",
    "def iou(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=num_classes)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou, np.mean(np.diag(iou))\n",
    "\n",
    "'''\n",
    "PIXEL ACCURACY\n",
    "'''\n",
    "def pixel_accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / y_true.size\n",
    "\n",
    "'''\n",
    "MEAN ACCURACY\n",
    "'''\n",
    "def mean_accuracy(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "\n",
    "    area_true[area_true == 0] = 1e-9\n",
    "    accuracy = np.diag(intersection) / area_true\n",
    "\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "'''\n",
    "FIRM-WEIGHT INTERSECTION OVER UNION\n",
    "'''\n",
    "def fw_iou(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=num_classes)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "    fw_iou = np.sum(area_true * iou) / np.sum(area_true)\n",
    "\n",
    "    return fw_iou\n",
    "\n",
    "def display_one_hot_annotation(annotations_onehot, num_classes):\n",
    "    label = np.argmax(annotations_onehot, axis=-1)\n",
    "    cmap = plt.get_cmap('tab10', num_classes)\n",
    "\n",
    "    plt.imshow(label, cmap=cmap)\n",
    "    plt.colorbar(ticks=range(num_classes), format=plt.FuncFormatter(lambda val, loc: {\n",
    "        0: \"unlabeled\",\n",
    "        1: \"sand\",\n",
    "        2: \"soil\",\n",
    "        3: \"ballast\",\n",
    "        4: \"rock\",\n",
    "        5: \"bedrock\",\n",
    "        6: \"rocky terrain\"\n",
    "    }[val]))\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CREATE DARKENED IMAGE PAIRS\n",
    "'''\n",
    "exp2_rgb_X_train_dark = []\n",
    "\n",
    "DARKEN_FACTOR = 0.5\n",
    "\n",
    "for n, image in tqdm(enumerate(exp2_rgb_X_train), total=len(exp2_rgb_X_train)):\n",
    "    darkened_image = image * DARKEN_FACTOR\n",
    "    exp2_rgb_X_train_dark.append(darkened_image)\n",
    "\n",
    "exp2_rgb_X_train_dark = np.array(exp2_rgb_X_train_dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a45e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SPLITTIG DATA SETS INTO BLOCKS\n",
    "'''\n",
    "SPLIT = 2\n",
    "\n",
    "'''\n",
    "TRAINING DATA SPLIT\n",
    "'''\n",
    "exp2_rgb_X_train_split = np.array_split(exp2_rgb_X_train, SPLIT)\n",
    "exp2_rgb_X_train_dark_split = np.array_split(exp2_rgb_X_train_dark, SPLIT)\n",
    "\n",
    "exp2_ir_X_train_split = np.array_split(exp2_ir_X_train, SPLIT)\n",
    "exp2_y_train_split = np.array_split(exp2_y_train, SPLIT)\n",
    "\n",
    "'''\n",
    "VALIDATION DATA\n",
    "'''\n",
    "exp2_rgb_X_val_split = np.array_split(exp2_rgb_X_val, SPLIT)\n",
    "exp2_ir_X_val_split = np.array_split(exp2_ir_X_val, SPLIT)\n",
    "exp2_y_val_split = np.array_split(exp2_y_val, SPLIT)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL PARAMS\n",
    "'''\n",
    "RGB_DIM = (572, 572, 3)\n",
    "IR_DIM = (572, 572)\n",
    "ANNOTATION_DIM = (572, 572, 7)\n",
    "CAT_DIM = (572, 572, 4)\n",
    "\n",
    "NUM_CLASSES = 7\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1000 \n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 15\n",
    "FACTOR = 0.1\n",
    "\n",
    "EXP2_FILENAME = \"tunet_siamesebl_exp2_batch{}_epoch{}_lr{}_p{}_f{}\".format(\n",
    "    BATCH_SIZE, EPOCHS, LEARNING_RATE, PATIENCE, FACTOR)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98e7ac",
   "metadata": {},
   "source": [
    "### TUNet BL Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL\n",
    "'''\n",
    "def ContractionPath(inputs, _padding='same', _activation='relu'):\n",
    "    c1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    c1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(0.25)(p1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p1)\n",
    "    c2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(0.5)(p2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(p2)\n",
    "    c3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(0.5)(p3)\n",
    "\n",
    "    c4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(p3)\n",
    "    c4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(c4)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(0.5)(p4)\n",
    "    \n",
    "    '''\n",
    "    RETURN\n",
    "    '''\n",
    "    return c1, c2, c3, c4, p4\n",
    "\n",
    "def ExpansionPath(c1, c2, c3, c4, p4, _padding='same', _activation='relu'):    \n",
    "    cm = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(p4)\n",
    "    cm = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(cm)\n",
    "    \n",
    "    deconv4 = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding=\"same\")(cm)\n",
    "    c4 = resize(c4, (deconv4.shape[1], deconv4.shape[2]), method=ResizeMethod.BILINEAR)\n",
    "    uconv4 = concatenate([deconv4, c4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    deconv3 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    c3 = resize(c3, (deconv3.shape[1], deconv3.shape[2]), method=ResizeMethod.BILINEAR)\n",
    "    uconv3 = concatenate([deconv3, c3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    deconv2 = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    c2 = resize(c2, (deconv2.shape[1], deconv2.shape[2]), method=ResizeMethod.BILINEAR)\n",
    "    uconv2 = concatenate([deconv2, c2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    deconv1 = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    c1 = resize(c1, (deconv1.shape[1], deconv1.shape[2]), method=ResizeMethod.BILINEAR)\n",
    "    uconv1 = concatenate([deconv1, c1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    \n",
    "    return uconv1\n",
    "\n",
    "def TUNet_BL():\n",
    "    '''\n",
    "    INPUT\n",
    "    '''\n",
    "    # Expected inputs are an RGB and an IR image\n",
    "    input_rgb = Input((572, 572, 3))\n",
    "    input_ir = Input((572, 572))\n",
    "    \n",
    "    # Concatenate for bottom layer input\n",
    "    input_ir = tf.expand_dims(input_ir, axis=-1)\n",
    "    cat_input = tf.concat([input_rgb, input_ir], axis=-1)\n",
    "    \n",
    "    '''\n",
    "    CONTRACTION PATH\n",
    "    '''\n",
    "    c1, c2, c3, c4, p4 = ContractionPath(cat_input)\n",
    "\n",
    "    '''\n",
    "    EXPANSION PATH\n",
    "    '''\n",
    "    cout = ExpansionPath(c1, c2, c3, c4, p4)\n",
    "\n",
    "    '''\n",
    "    OUTPUT\n",
    "    '''\n",
    "    softmax_output = Conv2D(7, (1, 1), padding=\"same\", activation='softmax')(cout)\n",
    "    softmax_output = resize(softmax_output, (572, 572), method=ResizeMethod.BILINEAR)\n",
    "\n",
    "    '''\n",
    "    RETURN\n",
    "    '''\n",
    "    return Model(inputs=[input_rgb, input_ir], outputs=softmax_output)\n",
    "\n",
    "def TUNet_SiameseBL():\n",
    "    '''\n",
    "    BASE MODELS\n",
    "    '''\n",
    "    b1 = TUNet_BL()\n",
    "    b2 = TUNet_BL()\n",
    "    \n",
    "    b2.set_weights(b1.get_weights())\n",
    "    \n",
    "    '''\n",
    "    INPUTS\n",
    "    '''\n",
    "    input_rgb_1 = Input((572, 572, 3))\n",
    "    input_ir_1 = Input((572, 572))\n",
    "    \n",
    "    input_rgb_2 = Input((572, 572, 3))\n",
    "    input_ir_2 = Input((572, 572))\n",
    "    \n",
    "    '''\n",
    "    OUTPUTS\n",
    "    '''\n",
    "    output1 = b1([input_rgb_1, input_ir_1])\n",
    "    output2 = b2([input_rgb_2, input_ir_2])\n",
    "    \n",
    "    '''\n",
    "    AVERAGE OUTPUT\n",
    "    '''\n",
    "    avg_output = Average()([output1, output2])\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[input_rgb_1, input_ir_1, input_rgb_2, input_ir_2], \n",
    "        outputs=[avg_output])\n",
    "    \n",
    "    '''\n",
    "    RETURN\n",
    "    '''\n",
    "    return model\n",
    "\n",
    "'''\n",
    "LOSS FUNCTIONS\n",
    "'''\n",
    "def global_MSE_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "    \n",
    "def total_loss(y_true, y_pred):\n",
    "    cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    mse_loss = global_MSE_loss(y_true, y_pred)\n",
    "    return cross_entropy_loss + 200.0 * mse_loss\n",
    "\n",
    "'''\n",
    "TRAIN\n",
    "'''\n",
    "def TUNet_Train(model, weights_filename, \n",
    "               X_rgb_train, X_rgb_train_dark, X_ir_train, y_train, \n",
    "               X_rgb_val, X_ir_val, y_val):\n",
    "\n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            \n",
    "            model.compile(\n",
    "                optimizer='sgd', \n",
    "                loss=total_loss, \n",
    "                metrics=['accuracy'])\n",
    "            \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(weights_filename, save_best_only=True, save_weights_only=True, verbose=1),\n",
    "                EarlyStopping(patience=PATIENCE, verbose=1),\n",
    "                ReduceLROnPlateau(factor=FACTOR, patience=PATIENCE, min_lr=LEARNING_RATE, verbose=1)]\n",
    "    \n",
    "            for i in range(SPLIT):\n",
    "                \n",
    "                print(\"\\n====== Batch \" + repr(i+1) + \" ======\\n\")\n",
    "                \n",
    "                history = model.fit(\n",
    "                    [X_rgb_train[i], X_ir_train[i], X_rgb_train_dark[i], X_ir_train[i]], \n",
    "                    y_train[i],\n",
    "                    validation_data=(\n",
    "                        [X_rgb_val[i], X_ir_val[i], X_rgb_val[i], X_ir_val[i]], \n",
    "                        y_val[i]),\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks, verbose=2)\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "'''\n",
    "SCORE\n",
    "'''\n",
    "def TUNet_Score(model, weights_filename, metrics_filename, \n",
    "                X_rgb_test, X_ir_test, y_test):\n",
    "    \n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            model.load_weights(weights_filename)\n",
    "\n",
    "            score = model.evaluate(\n",
    "                [X_rgb_test, X_ir_test, X_rgb_test, X_ir_test], \n",
    "                [y_test], \n",
    "                batch_size=2, verbose=1)\n",
    "\n",
    "            print(\"Test loss:\", score[0])\n",
    "            print(\"Test accuracy:\", score[1])\n",
    "\n",
    "            with open(metrics_filename, \"a\") as f:\n",
    "                f.write(f\"\\Test Loss: {score[0]}\\n\")\n",
    "                f.write(f\"\\Test Accuracy: {score[1]}\\n\")\n",
    "                \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "'''\n",
    "PREDICT\n",
    "'''\n",
    "def TUNet_Predict(model, X_rgb_test, X_ir_test):\n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            pred = model.predict(\n",
    "                [X_rgb_test, X_ir_test, X_rgb_test, X_ir_test], \n",
    "                batch_size=2)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "'''\n",
    "DISPLAY RANDOM RESULT\n",
    "'''\n",
    "def TUNet_Display(X_rgb_test, X_ir_test, y_test, y_pred):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 6))\n",
    "    n = random.randint(0, len(X_rgb_test)-1)\n",
    "    cmap = plt.get_cmap('tab10', 7)\n",
    "    \n",
    "    axes[0].imshow(X_rgb_test[n])\n",
    "    axes[1].imshow(X_ir_test[n], cmap='gray')\n",
    "    axes[2].imshow(np.argmax(y_test[n], axis=-1), cmap=cmap)\n",
    "    axes[3].imshow(np.argmax(y_pred[n], axis=-1), cmap=cmap)\n",
    "                                           \n",
    "    axes[0].set_title(\"RGB\")\n",
    "    axes[1].set_title(\"IR\")\n",
    "    axes[2].set_title(\"Annotation\")\n",
    "    axes[3].set_title(\"Predicted\")\n",
    "    \n",
    "    for ax in axes.flatten():\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "'''\n",
    "CALCULATE METRICS\n",
    "'''\n",
    "def TUNet_Metrics(y_test, y_pred, metrics_filename):\n",
    "    y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "    y_true_classes = np.argmax(y_test, axis=-1)\n",
    "    \n",
    "    num_classes = 7\n",
    "    \n",
    "    iou_values, mean_iou = iou(y_true_classes, y_pred_classes, num_classes)\n",
    "    pixel_acc = pixel_accuracy(y_true_classes, y_pred_classes)\n",
    "    mean_acc = mean_accuracy(y_true_classes, y_pred_classes, num_classes)\n",
    "    fw_iou_value = fw_iou(y_true_classes, y_pred_classes, num_classes)\n",
    "    \n",
    "    print(f\"Mean IoU: {mean_iou}\")\n",
    "    print(f\"Pixel accuracy: {pixel_acc}\")\n",
    "    print(f\"Mean accuracy: {mean_acc}\")\n",
    "    print(f\"Frequency-Weighted IoU: {fw_iou_value}\")\n",
    "    \n",
    "    with open(metrics_filename, \"a\") as f:\n",
    "        f.write(\"\\nIoU Values:\\n\")\n",
    "        \n",
    "        for i, iou_val in enumerate(iou_values):\n",
    "            f.write(f\"Class {i}: {iou_val}\\n\")\n",
    "        f.write(f\"\\nMean IoU: {mean_iou}\\n\")\n",
    "        f.write(f\"Pixel Accuracy: {pixel_acc}\\n\")\n",
    "        f.write(f\"Mean Accuracy: {mean_acc}\\n\")\n",
    "        f.write(f\"Frequency Weighted IoU: {fw_iou_value}\\n\")\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba66ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with tf.device(gpu_p40):\n",
    "        exp2_model = TUNet_SiameseBL()\n",
    "            \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a0305",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNet_Train(exp2_model, os.path.join(WEIGHTS_PATH, EXP2_FILENAME + '.h5'), \n",
    "            exp2_rgb_X_train_split, exp2_rgb_X_train_dark_split, exp2_ir_X_train_split, exp2_y_train_split,\n",
    "            exp2_rgb_X_val_split, exp2_ir_X_val_split, exp2_y_val_split)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ff05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = TUNet_Score(\n",
    "    exp2_model, \n",
    "    os.path.join(WEIGHTS_PATH, EXP2_FILENAME + '.h5'),\n",
    "    os.path.join(METRICS_PATH, EXP2_FILENAME + '.txt'),\n",
    "    exp2_rgb_X_test,\n",
    "    exp2_ir_X_test,\n",
    "    exp2_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3074a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = TUNet_Predict(exp2_model, exp2_rgb_X_test, exp2_ir_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNet_Display(exp2_rgb_X_test, exp2_ir_X_test, exp2_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4aa047",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNet_Metrics(exp2_y_test, y_pred, os.path.join(METRICS_PATH, EXP2_FILENAME + '.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7123c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
